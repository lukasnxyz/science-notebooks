{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b405317b-831a-4eca-aace-05b7f157811b",
   "metadata": {},
   "source": [
    "- testing how well text embedding models can encode the features of input\n",
    "- goal is to have some sort of custom benchmark for this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "feb99ef8-04ae-4547-a5f5-b8d792536d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x10990d6d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "torch.manual_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d021b45-54b2-4328-a349-cc6eaf8eb4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(texts):\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0]  # CLS token pooling\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=1)\n",
    "    return embeddings\n",
    "\n",
    "def cosine_sim(embeddings):\n",
    "    n = embeddings.size()[0]\n",
    "    sim = lambda a, b: torch.dot(a, b) / torch.linalg.norm(a) * torch.linalg.norm(b)\n",
    "    sims = torch.empty(n, n)\n",
    "    for i, e in enumerate(embeddings):\n",
    "        for ii, ee, in enumerate(embeddings):\n",
    "            sims[i, ii] = sim(e, ee)\n",
    "    return sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eada44da-1848-4445-96e6-324845704b64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaModel(\n",
       "  (embeddings): RobertaEmbeddings(\n",
       "    (word_embeddings): Embedding(50265, 384, padding_idx=1)\n",
       "    (position_embeddings): Embedding(514, 384, padding_idx=1)\n",
       "    (token_type_embeddings): Embedding(2, 384)\n",
       "    (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): RobertaEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-5): 6 x RobertaLayer(\n",
       "        (attention): RobertaAttention(\n",
       "          (self): RobertaSdpaSelfAttention(\n",
       "            (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): RobertaSelfOutput(\n",
       "            (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "            (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): RobertaIntermediate(\n",
       "          (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): RobertaOutput(\n",
       "          (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          (LayerNorm): LayerNorm((384,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): RobertaPooler(\n",
       "    (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"ibm-granite/granite-embedding-30m-english\"\n",
    "model = AutoModel.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f49491e3-f614-4477-9f32-83676792d374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(120000,\n",
       " [\"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\",\n",
       "  'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here labels are world(0), sports(1), business(2), sci/tech(3)\n",
    "dataset = load_dataset(\"ag_news\")\n",
    "len(dataset[\"train\"]['text']), dataset[\"train\"]['text'][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea6ff18a-c420-40af-89a7-15c8d0fc6d37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.7019, 0.7057, 0.6213, 0.6776],\n",
       "        [0.7019, 1.0000, 0.6954, 0.6464, 0.6647],\n",
       "        [0.7057, 0.6954, 1.0000, 0.7319, 0.7669],\n",
       "        [0.6213, 0.6464, 0.7319, 1.0000, 0.7396],\n",
       "        [0.6776, 0.6647, 0.7669, 0.7396, 1.0000]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing sim embeddings\n",
    "texts = dataset[\"train\"]['text'][0:5]\n",
    "embeddings = embed(texts)\n",
    "\n",
    "sim_matrix = cosine_sim(embeddings)\n",
    "sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9bbb7568-ed01-4ab5-b8fe-c7f0f0f5c132",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([153190, 135656,  34323,  44974,  32566,  82318, 142913,  36256,   9704,\n",
       "        123425,  10731, 130870, 153190, 137429,  57832, 104235, 148767,  30077,\n",
       "        128725,  72057,  25840,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0,      0,      0,      0,\n",
       "             0,      0,      0,      0,      0,      0])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_texts = dataset[\"train\"][\"text\"]\n",
    "vocab = sorted(set(word.lower() for text in train_texts for word in text.split()))\n",
    "word2idx = {w: i for i, w in enumerate(vocab)}\n",
    "\n",
    "# Convert text -> token IDs\n",
    "token_ids = [torch.tensor([word2idx[w.lower()] for w in text.split()]) for text in train_texts]\n",
    "\n",
    "# Pad sequences to same length\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "token_ids_padded = pad_sequence(token_ids, batch_first=True)\n",
    "token_ids_padded[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bd5c83fa-f2bc-4bab-8793-d091e1b16e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fw/vpvz28gx2fgffx2m8nf13j1h0000gn/T/ipykernel_33748/4060491472.py:9: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  return torch.tensor(embeddings, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)\n"
     ]
    }
   ],
   "source": [
    "def embed_split(split):\n",
    "    texts = list(dataset[split][\"text\"][0:4000])\n",
    "    labels = list(dataset[split][\"label\"][0:4000])\n",
    "    inputs = tokenizer(texts, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state[:, 0] # CLS token pooling\n",
    "        embeddings = torch.nn.functional.normalize(embeddings, dim=1)\n",
    "    return torch.tensor(embeddings, dtype=torch.float32), torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "X_train, y_train = embed_split(\"train\")\n",
    "X_test, y_test = embed_split(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ffe485df-3677-4549-a73f-b6899aa4436b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 384)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds = TensorDataset(X_train, y_train)\n",
    "test_ds = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=256)\n",
    "\n",
    "num_classes = len(set(dataset[\"train\"][\"label\"]))\n",
    "input_dim = X_train.shape[1]\n",
    "num_classes, input_dim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4e0203-c207-4010-bce8-06b2edb801b4",
   "metadata": {},
   "source": [
    "- using linear probing (traing a linear layer using logistic regression on the frozen embeddings) to check how well the embedding models encoded the text\n",
    "- the goals is for the linear probing layer to learn the embeddings and their features\n",
    "- random guessing for a dataset like ag news with 4 classes would be 25% so something like 87% is pretty good\n",
    "- linear probing score == (kinda) compression efficiency score\n",
    "- more so though, linear probing also checks the linear seperability between features in the embeddings. to check if the features are even there at all, it would make more sense to use non-linear probing with something like a simple 2 layer mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7fdcbd28-6da5-4440-b304-ddfcc651e631",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogisticRegression(nn.Module):\n",
    "    def __init__(self, input_dim: int, num_classes: int):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(input_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "def train(probe, loss_fn, optimizer, train_loader, epochs):\n",
    "    probe.train()\n",
    "    for epoch in range(epochs):\n",
    "        for xb, yb in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = probe(xb)\n",
    "            loss = loss_fn(logits, yb)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        if epoch % (epochs // 10) == 0:\n",
    "            print(f\"epoch {epoch+1}/{epochs}, loss: {loss.item():.4f}\")\n",
    "\n",
    "def eval(probe, test_loader):\n",
    "    probe.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in test_loader:\n",
    "            logits = probe(xb)\n",
    "            preds = torch.argmax(logits, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(yb.cpu().numpy())\n",
    "    return all_preds, all_labels\n",
    "\n",
    "def accuracy(all_preds, all_labels):\n",
    "    total_acc = 0\n",
    "    for p, l in zip(all_preds, all_labels):\n",
    "        if p == l: total_acc += 1\n",
    "    return total_acc / len(all_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "729b7911-b261-46ff-a795-348812dc5fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1/400, loss: 1.3183\n",
      "epoch 41/400, loss: 0.3868\n",
      "epoch 81/400, loss: 0.4437\n",
      "epoch 121/400, loss: 0.3236\n",
      "epoch 161/400, loss: 0.2183\n",
      "epoch 201/400, loss: 0.2687\n",
      "epoch 241/400, loss: 0.1063\n",
      "epoch 281/400, loss: 0.3342\n",
      "epoch 321/400, loss: 0.3588\n",
      "epoch 361/400, loss: 0.3281\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.864"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probe = LogisticRegression(input_dim, num_classes)\n",
    "loss_fn = nn.CrossEntropyLoss(reduction=\"mean\")\n",
    "optimizer = torch.optim.Adam(probe.parameters(), lr=1e-3)\n",
    "epochs = 400\n",
    "\n",
    "total_loss = train(probe, loss_fn, optimizer, train_loader, epochs)\n",
    "all_preds, all_labels = eval(probe, test_loader)\n",
    "accuracy(all_preds, all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbb866-4941-45d6-9ad5-066ac18aff7d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d45f638-8a33-40d4-a70d-73409e912e66",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
