{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8b3049bd-a7dd-4471-a490-9a434bc53495",
   "metadata": {},
   "source": [
    "### Introduction to Information Theory\n",
    "Invented by Claude E. Shannon. https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf\n",
    "\n",
    "#### The Basic Idea of Shannon Entropy\n",
    "Entropy quantifies the average amount of information inherent in a possible set of outcomes of a random variable. A random variable is a function that assigns a numerical value to each possible outcome of a random experiment.\n",
    "\n",
    "$\n",
    "H(X) = -\\sum_x (p(x) \\cdot \\log_2{p(x)})\n",
    "$\n",
    "\n",
    "$\\log_2$ is usually chosen to for representing information in binary notation. This formula is used to determine the amount of information needed to represent the outcome of a random experiment. In this case, the amount is quantified using bits (because $\\log_2$) so the output would be the number of bits needed to represent the output.\n",
    "\n",
    "#### An Example\n",
    "Given a fair coin with the probability of heads being $p = 0.5$ and the probability of tails being $p = 0.5$, you can calculate how many bits are needed to represent the outcome of running the random experiemnt of flipping the coin once.\n",
    "\n",
    "$\n",
    "h = -(0.5 \\cdot \\log_2{0.5} + 0.5 \\cdot \\log_2{0.5}) = -(0.5 \\cdot (-1) + 0.5 \\cdot (-1)) = 1 bit\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b7e73c19-6378-444a-8bfe-e5363d22bc8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cc8f6db-f97d-42a6-993c-d8e62617d079",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I(X) = -log_2(p)\n",
    "# the bits of information we receive for an event X\n",
    "#    p being the probability of the event X happening\n",
    "I = lambda X: -np.log2(1 / 2**(len(X)))\n",
    "I(\"0010\") # bits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "296bd693-56c6-4015-9987-01ed5c0666f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(6.0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only captures the information of a single discrete event\n",
    "def self_information(p):\n",
    "    return -np.log2(p)\n",
    "self_information(1 / 64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a5474b-ad3e-4e81-bc42-0d70b07cf291",
   "metadata": {},
   "source": [
    "#### shannon entropy\n",
    "1. the information we gain by observing a random variables does not depend on what we call the elemnts or the presence of additional elemnts which have a probability of zero.\n",
    "2. the information we gain by observing two random variables is no more than the sum of the information we gain by observing them separately. if they are independent, then it is exactly the sum.\n",
    "3. the information gained when observing (nearly) certain events is (nearly) zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06545b4-3a9b-41e8-b2e0-4d2a5f6cbcbc",
   "metadata": {},
   "source": [
    "- H(X) = -E_x~p\\[log2 p(x)\\]\n",
    "- if x is discrete, we sum, if its continuous we integrate\n",
    "- a llog can naturally converta a probability distribution product to a sum of the individual parts\n",
    "- negative log because any log of \\[0,1\\] is negative so want to make it positive\n",
    "- the likley hood of events and their entropy should be monotonically and decreasingly related because we want the more rare events to have higher entropy than the more common ones as we gain more information from the more rare events (since we already have information from the common ones)\n",
    "- the \"surprise\" grows limitless as the likelihood approaches 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60608254-cd00-46ce-b600-822108face9d",
   "metadata": {},
   "source": [
    "#### entropy of a slot machine\n",
    "- with s1...sk symbols and p1...pk probabilites for those symbols\n",
    "$\n",
    "H(S) = \\sum_i p_i \\cdot I(s_i) = - \\sum_i p_i \\cdot \\log p_i\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cbfee8-1521-4d7b-96a3-6e7a7e701287",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
