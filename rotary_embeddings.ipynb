{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "82f4af90-3f33-4da1-a65c-d0e8fd50faab",
   "metadata": {},
   "source": [
    "- here's the [og paper](https://arxiv.org/pdf/2104.09864) on rotary positional embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82041a67-6f1e-4fa4-9a5a-1bfc06a39817",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from typing import Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b34ad094-e288-4580-b264-500a95f3bb9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freqs_cis.size() torch.Size([2048, 32])\n",
      "xq.size() torch.Size([2, 2048, 64]) xk.size() torch.Size([2, 2048, 64])\n",
      "xq_r.size() torch.Size([2, 2048, 64]) xk_r.size() torch.Size([2, 2048, 64])\n",
      "Expected xq_r[0, m, 2k]: -0.31764817237854004\n",
      "Actual xq_r[0, m, 2k]: -0.31764817237854004\n",
      "Expected xq_r[0, m, 2k+1]: -0.4906976521015167\n",
      "Actual xq_r[0, m, 2k+1]: -0.4906976521015167\n"
     ]
    }
   ],
   "source": [
    "# functions are from the llama 3 model implementation (https://github.com/meta-llama/llama3/blob/main/llama/model.py)\n",
    "def precompute_freqs_cis(dim: int, end: int, theta: float = 10000.0):\n",
    "    freqs = 1.0 / (theta ** (torch.arange(0, dim, 2)[: (dim // 2)].float() / dim))\n",
    "    t = torch.arange(end, device=freqs.device, dtype=torch.float32)\n",
    "    freqs = torch.outer(t, freqs)\n",
    "    freqs_cis = torch.polar(torch.ones_like(freqs), freqs)  # complex64\n",
    "    return freqs_cis  \n",
    "\n",
    "def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n",
    "    ndim = x.ndim\n",
    "    assert 0 <= 1 < ndim\n",
    "    assert freqs_cis.shape == (x.shape[1], x.shape[-1]), f\"{freqs_cis.shape} != ({x.shape[1]}, {x.shape[-1]})\"\n",
    "    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n",
    "    return freqs_cis.view(*shape)\n",
    "\n",
    "def apply_rotary_emb(\n",
    "    xq: torch.Tensor,\n",
    "    xk: torch.Tensor,\n",
    "    freqs_cis: torch.Tensor,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))\n",
    "    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2))\n",
    "    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)\n",
    "    xq_out = torch.view_as_real(xq_ * freqs_cis).reshape(*xq.shape[:-1], -1)\n",
    "    xk_out = torch.view_as_real(xk_ * freqs_cis).reshape(*xk.shape[:-1], -1)\n",
    "    return xq_out.type_as(xq), xk_out.type_as(xk)\n",
    "\n",
    "dim = 2048 // 32 \n",
    "max_seq_len = 1024 * 2\n",
    "theta = 500000.0\n",
    "batch_size = 2\n",
    "seq_len = 1024\n",
    "embed_dim = dim\n",
    "\n",
    "freqs_cis = precompute_freqs_cis(dim, max_seq_len, theta)\n",
    "\n",
    "print(\"freqs_cis.size()\", freqs_cis.size())\n",
    "\n",
    "xq = torch.randn(batch_size, max_seq_len, 64)\n",
    "xk = torch.randn(batch_size, max_seq_len, 64)\n",
    "\n",
    "print(\"xq.size()\", xq.size(), \"xk.size()\", xk.size())\n",
    "\n",
    "(xq_r, xk_r) = apply_rotary_emb(xq, xk, freqs_cis)\n",
    "print(\"xq_r.size()\", xq_r.size(), \"xk_r.size()\", xk_r.size())\n",
    "\n",
    "# | verifying rotary embedding\n",
    "\n",
    "m = 100 # position to check\n",
    "k = 0 # first dimension pair (0, 1)\n",
    "\n",
    "# compute frequency\n",
    "freq = 1.0 / (theta ** (2 * k / dim))  # θ_k\n",
    "angle = m * freq  # m * θ_k\n",
    "\n",
    "# get input values\n",
    "x_2k = xq[0, m, 2 * k]\n",
    "x_2k1 = xq[0, m, 2 * k + 1]\n",
    "\n",
    "# expected output after rotation\n",
    "cos_angle = torch.cos(torch.tensor(angle))\n",
    "sin_angle = torch.sin(torch.tensor(angle))\n",
    "xq_r_expected_2k = x_2k * cos_angle - x_2k1 * sin_angle\n",
    "xq_r_expected_2k1 = x_2k * sin_angle + x_2k1 * cos_angle\n",
    "\n",
    "# compare with actual output\n",
    "print(\"Expected xq_r[0, m, 2k]:\", xq_r_expected_2k.item())\n",
    "print(\"Actual xq_r[0, m, 2k]:\", xq_r[0, m, 2 * k].item())\n",
    "print(\"Expected xq_r[0, m, 2k+1]:\", xq_r_expected_2k1.item())\n",
    "print(\"Actual xq_r[0, m, 2k+1]:\", xq_r[0, m, 2 * k + 1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ee2f51-0e91-4901-8d0c-313375dc2b37",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": ".venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
